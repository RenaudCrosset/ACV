{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaae2d35",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <p style=\"text-align:center; color:blue; font-size:26px; font-style:italic;\">Advanced Computer Vision Course</p>\n",
    "<div/>\n",
    "    <div align=right>Etienne Balt<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623a4251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 10]  # Make inline plots larger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1100403a",
   "metadata": {},
   "source": [
    "\n",
    "## Image Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3756c5d",
   "metadata": {},
   "source": [
    "### Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e99aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalefactor=10\n",
    "\n",
    "messi = plt.imread('data/messi.jpg')\n",
    "\n",
    "#the heart of the code:\n",
    "res = cv2.resize(messi,None,fx=scalefactor, fy=scalefactor,\n",
    "                 interpolation = cv2.INTER_LANCZOS4)\n",
    "\n",
    "#the remainder of the code is to generate the display image\n",
    "height, width = messi.shape[:2]\n",
    "\n",
    "bigwidth = int( np.amax( [scalefactor*height , height]) )\n",
    "bigheight = int( width*(1+scalefactor))\n",
    "\n",
    "bigim = np.zeros((bigwidth , bigheight ,3), np.uint8)\n",
    "bigim[:height,:width] = messi\n",
    "\n",
    "if scalefactor > 1:\n",
    "    bigim[:,width:] = res\n",
    "else:\n",
    "    bigim[: int(scalefactor*height),width:] = res\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(bigim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5497d360",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac7af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows,cols,chans = messi.shape\n",
    "\n",
    "xshift=100\n",
    "yshift=20\n",
    "\n",
    "M = np.float32([[1,0,xshift],[0,1,yshift]])\n",
    "dst = cv2.warpAffine(messi, M,(cols,rows))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93afc811",
   "metadata": {},
   "source": [
    "### Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23860a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messi_alpha = cv2.cvtColor(messi, cv2.COLOR_RGB2RGBA)\n",
    "\n",
    "rows, cols, chans = messi.shape\n",
    "\n",
    "#specify our rotation in degrees\n",
    "theta = 45.0\n",
    "\n",
    "M = cv2.getRotationMatrix2D((cols/2,rows/2),theta,1) \n",
    "dst = cv2.warpAffine(messi_alpha,M,(cols,rows), borderMode=cv2.BORDER_TRANSPARENT)\n",
    "                 \n",
    "plt.figure()\n",
    "plt.imshow(dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584449db",
   "metadata": {},
   "source": [
    "### General affine transform\n",
    "\n",
    "Map 3 input (x,y) points to 3 output points (ie triangle to triangle)\n",
    "\n",
    "Will perform translation, scaling, rotation, and skew, but not perspective correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301713bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows,cols,ch = messi.shape\n",
    "\n",
    "pts1 = np.float32([[0,0],[0,cols],[rows,0]])\n",
    "pts2 = np.float32([[10,50],[40,cols-40],[rows-100,30]])\n",
    "\n",
    "M = cv2.getAffineTransform(pts1,pts2)\n",
    "\n",
    "dst = cv2.warpAffine(messi,M,(cols,rows))\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(121),plt.imshow(messi),plt.title('Input')\n",
    "plt.subplot(122),plt.imshow(dst),plt.title('Output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb80fb85",
   "metadata": {},
   "source": [
    "### Perspective transform\n",
    "\n",
    "4 point transform\n",
    "\n",
    "Nonlinear but more general than affine\n",
    "\n",
    "Perspective:  further away objects look smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13411ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imread('data/right.jpg')\n",
    "rows,cols,chans = img.shape\n",
    "\n",
    "startpts =  np.float32([[362, 107], [530,139] , [453,416], [319,337]])\n",
    "endpts   =  np.float32([[100, 100], [330,100],  [330,400], [100,400]])\n",
    "\n",
    "M = cv2.getPerspectiveTransform(startpts,endpts)\n",
    "\n",
    "dst = cv2.warpPerspective(img, M, (rows,cols))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(dst[100:400,100:330])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c393ad3-3d76-4a53-bf17-bab8a8c534fd",
   "metadata": {},
   "source": [
    "# Image compositing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c2c20c",
   "metadata": {},
   "source": [
    "### Combine images using a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d1bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "star_mask = plt.imread('data/star_mask.jpg').astype('float') / 255.\n",
    "\n",
    "baboon = plt.imread('data/baboon.jpg').astype('float')\n",
    "resized_baboon = cv2.resize(baboon, star_mask.shape[1::-1])\n",
    "\n",
    "masked = (star_mask * resized_baboon).astype('uint8')\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(masked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab891c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "star_mask = plt.imread('data/star_mask.jpg').astype('float') / 255.\n",
    "\n",
    "baboon = plt.imread('data/baboon.jpg').astype('float')\n",
    "resized_baboon = cv2.resize(baboon, star_mask.shape[1::-1])\n",
    "\n",
    "home = plt.imread('data/home.jpg').astype('float')\n",
    "resized_home = cv2.resize(home, star_mask.shape[1::-1])\n",
    "\n",
    "masked = (star_mask * resized_baboon + (1. - star_mask)*resized_home).astype('uint8')\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831119a6",
   "metadata": {},
   "source": [
    "### Seamless copy (advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8cc2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv2.imread(\"data/wood-texture.jpg\")\n",
    "\n",
    "\n",
    "obj = cv2.imread(\"data/iloveyouticket.jpg\")\n",
    "\n",
    "mask = 255 * np.ones(obj.shape, obj.dtype)\n",
    "\n",
    "width, height, channels = im.shape\n",
    "center = (height//2, width//2)\n",
    "    \n",
    "seamless = cv2.seamlessClone(obj, im, mask, center, cv2.MIXED_CLONE)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cv2.cvtColor(seamless, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afc4600-ef78-4d01-b45b-d2cd72602030",
   "metadata": {},
   "source": [
    "## Mediapipe presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b204f2f3-301c-4925-915d-44c0f5ec09f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "\n",
    "drawing_utils = mp.solutions.drawing_utils\n",
    "drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4848e3-bfe0-4c3d-a68b-f1d2af2fd6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_filter_with_mediapipe_model(mediapipe_model, mediapipe_based_filter):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    with mediapipe_model as model:\n",
    "        while cap.isOpened():\n",
    "            success, image = cap.read()\n",
    "\n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                continue     # If loading a video, use 'break' instead of 'continue'.\n",
    "\n",
    "            # Flip the image horizontally for a later selfie-view display, and convert\n",
    "            # the BGR image to RGB.\n",
    "            image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            results = model.process(image)\n",
    "\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            result_image = mediapipe_based_filter(image, results)\n",
    "\n",
    "            cv2.imshow('MediaPipe', result_image)\n",
    "\n",
    "            if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197d4dad-3d9a-4850-84be-6ba4b66a5f07",
   "metadata": {},
   "source": [
    "### Holistic (Hands, Face and Pose tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99611a2-17eb-4378-8432-de0d4510e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Holistic = mp.solutions.holistic.Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9284aed-e8a8-49d4-b74b-85144386b9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_holistic_results(image, results, show_hands=True, show_face=True, show_pose=False):\n",
    "    if show_hands:\n",
    "        drawing_utils.draw_landmarks(\n",
    "            image,\n",
    "            results.left_hand_landmarks,\n",
    "            mp.solutions.holistic.HAND_CONNECTIONS,\n",
    "            connection_drawing_spec=drawing_styles.get_default_hand_connections_style()\n",
    "        )\n",
    "\n",
    "        drawing_utils.draw_landmarks(\n",
    "            image,\n",
    "            results.right_hand_landmarks,\n",
    "            mp.solutions.holistic.HAND_CONNECTIONS,\n",
    "            connection_drawing_spec=drawing_styles.get_default_hand_connections_style()\n",
    "        )\n",
    "\n",
    "    if show_face:\n",
    "        drawing_utils.draw_landmarks(\n",
    "            image,\n",
    "            results.face_landmarks,\n",
    "            mp.solutions.holistic.FACEMESH_CONTOURS,\n",
    "            landmark_drawing_spec=drawing_utils.DrawingSpec(thickness=0, circle_radius=0, color=(255, 255, 255)),\n",
    "            connection_drawing_spec=drawing_styles.get_default_face_mesh_contours_style()\n",
    "        )\n",
    "\n",
    "    if show_pose:\n",
    "        drawing_utils.draw_landmarks(\n",
    "            image,\n",
    "            results.pose_landmarks,\n",
    "            mp.solutions.holistic.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=drawing_styles.get_default_pose_landmarks_style()\n",
    "        )\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c2ede-adb3-4f31-9783-49ab748e9502",
   "metadata": {},
   "outputs": [],
   "source": [
    "holistic_model = Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08344a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_image, last_results = run_filter_with_mediapipe_model(mediapipe_model=holistic_model,\n",
    "                                                           mediapipe_based_filter=draw_holistic_results\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ca616d",
   "metadata": {},
   "source": [
    "# Mes fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab19897",
   "metadata": {},
   "source": [
    "<a ref=\"https://github.com/google/mediapipe/blob/a908d668c730da128dfa8d9f6bd25d519d006692/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca74b1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9390c532",
   "metadata": {},
   "source": [
    "<img alt=\"Face\" src=\"https://raw.githubusercontent.com/google/mediapipe/a908d668c730da128dfa8d9f6bd25d519d006692/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png\" width=\"300\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2223fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6abc112",
   "metadata": {},
   "source": [
    "image fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e6dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_url = \"https://1vw4gb3u6ymm1ev2sp2nlcxf-wpengine.netdna-ssl.com/wp-content/uploads/shutterstock_149962697-946x658.jpg\"\n",
    "urllib.request.urlretrieve(face_url, \"face_image.jpg\")\n",
    "\n",
    "img = Image.open('face_image.jpg')\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b104a801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image filename and drawing specifications\n",
    "file = 'face_image.jpg'\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "# Create a face mesh object\n",
    "with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True,\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5) as face_mesh:\n",
    "\n",
    "    # Read image file with cv2 and process with face_mesh\n",
    "    image = cv2.imread(file)\n",
    "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Define boolean corresponding to whether or not a face was detected in the image\n",
    "face_found = bool(results.multi_face_landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c63324",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dd0ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if face_found:\n",
    "    # Create a copy of the image\n",
    "    annotated_image = image.copy()\n",
    "    \n",
    "    # Draw landmarks on face\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=results.multi_face_landmarks[0],\n",
    "        connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "            .get_default_face_mesh_tesselation_style())\n",
    "        \n",
    "    # Save image\n",
    "    cv2.imwrite('face_tesselation_only.png', annotated_image)\n",
    "\n",
    "# Open image\n",
    "img = Image.open('face_tesselation_only.png')\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1feab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df5e33a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "acf8df05f99cec74ae264b588ee44dea46fb74c08ab3831061232adefb5eaaf3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
