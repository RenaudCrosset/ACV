{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0587d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb0a8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import cv2\n",
    "import acv_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd45370",
   "metadata": {},
   "source": [
    "# Moustache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f77271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import acv_functions\n",
    "acv_functions.moustache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f044248",
   "metadata": {},
   "source": [
    "# Nez rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1eba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import acv_functions\n",
    "acv_functions.nez_rouge()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d818edd3",
   "metadata": {},
   "source": [
    "# Nez rouge r√©glable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b80a56cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import acv_functions\n",
    "acv_functions.nez_rouge_reglable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038cf1e5",
   "metadata": {},
   "source": [
    "# Nez rouge et crottes de nez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a4592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "goutte = plt.imread('data/goutte.jpg')\n",
    "plt.figure()\n",
    "plt.imshow(goutte);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5bed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "effect = 'crotte_nez_v2' # 'nez_rouge' ou 'crotte_nez' ou 'crotte_nez_v2'\n",
    "\n",
    "# For webcam input:\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "cap = cv2.VideoCapture(0)\n",
    "max_num_faces = 5\n",
    "\n",
    "with mp_face_mesh.FaceMesh(max_num_faces=max_num_faces,\n",
    "                           refine_landmarks=True, \n",
    "                           min_detection_confidence=0.5, \n",
    "                           min_tracking_confidence=0.5\n",
    "                          )as face_mesh:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue  # If loading a video, use 'break' instead of 'continue'\n",
    "        # To improve performance, optionally mark the image as not writeable to pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(image)\n",
    "\n",
    "        # Draw the face mesh annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        if results.multi_face_landmarks:\n",
    "            color = (0, 0, 255)\n",
    "            height, width, _ = image.shape\n",
    "            if effect == 'nez_rouge':\n",
    "                for nb_face in range(len(results.multi_face_landmarks)):\n",
    "                    nose = results.multi_face_landmarks[nb_face].landmark[4]\n",
    "                    cv2.circle(image, (int(nose.x*width), int(nose.y*height)), int(-400*nose.z+1), color, -1)\n",
    "            elif effect == 'crotte_nez':\n",
    "                obj = cv2.imread(\"data/petite_goutte.jpg\") #morve_square.jpg\")\n",
    "                for nb_face in range(len(results.multi_face_landmarks)):\n",
    "                    under_nose = results.multi_face_landmarks[nb_face].landmark[79]\n",
    "                    scalefactor = 1 - 2*under_nose.z\n",
    "                    obj = cv2.resize(obj, None, fx=1, fy=scalefactor, interpolation=cv2.INTER_LANCZOS4)\n",
    "                    mask = 255 * np.ones(obj.shape, obj.dtype)\n",
    "                    width, height, channels = image.shape\n",
    "                    center = (int(under_nose.x*height), int(under_nose.y*width) + obj.shape[0]//2)\n",
    "                try:\n",
    "                    image = cv2.seamlessClone(obj, image, mask, center, cv2.MIXED_CLONE)\n",
    "                except:\n",
    "                    pass\n",
    "          \n",
    "            elif effect == 'crotte_nez_v2':\n",
    "                obj = cv2.imread(\"data/petite_goutte.jpg\") #morve_square.jpg\")\n",
    "                obj_h, obj_w, _ = obj.shape\n",
    "                for nb_face in range(len(results.multi_face_landmarks)):\n",
    "                    try:\n",
    "                        under_nose = results.multi_face_landmarks[nb_face].landmark[79]\n",
    "                        image_crop = image[int(under_nose.y*height):int(under_nose.y*height)+obj_h,\n",
    "                                           int(under_nose.x*width)-obj_w//2:int(under_nose.x*width)+obj_w//2, :]\n",
    "                        image_crop_fusion = cv2.addWeighted(image_crop, 0.5, obj, 0.5, 0)\n",
    "                        image[int(under_nose.y*height):int(under_nose.y*height)+obj_h,\n",
    "                              int(under_nose.x*width)-obj_w//2:int(under_nose.x*width)+obj_w//2, :] = image_crop_fusion\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "        '''\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "        mp_drawing.draw_landmarks(image=image,\n",
    "                                  landmark_list=face_landmarks,\n",
    "                                  connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                                  landmark_drawing_spec=None,\n",
    "                                  connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "        mp_drawing.draw_landmarks(image=image,\n",
    "                                  landmark_list=face_landmarks,\n",
    "                                  connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                                  landmark_drawing_spec=None,\n",
    "                                  connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n",
    "        mp_drawing.draw_landmarks(image=image,\n",
    "                                  landmark_list=face_landmarks,\n",
    "                                  connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "                                  landmark_drawing_spec=None,\n",
    "                                  connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_iris_connections_style())\n",
    "        '''\n",
    "\n",
    "        # Flip the image horizontally for a selfie-view display.\n",
    "        cv2.imshow('MediaPipe Face Mesh', cv2.flip(image, 1))\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1ce4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df1f6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d941d9c",
   "metadata": {},
   "source": [
    "# Essai 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2045ee00",
   "metadata": {},
   "source": [
    "https://www.assemblyai.com/blog/mediapipe-for-dummies/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f44315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import PyQt5\n",
    "from PIL import Image\n",
    "from IPython.display import Video\n",
    "import nb_helpers\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_face_mesh = mp.solutions.face_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe503188",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_url = \"https://1vw4gb3u6ymm1ev2sp2nlcxf-wpengine.netdna-ssl.com/wp-content/uploads/shutterstock_149962697-946x658.jpg\"\n",
    "urllib.request.urlretrieve(face_url, \"face_image.jpg\")\n",
    "\n",
    "img = Image.open('face_image.jpg')\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a948ef",
   "metadata": {},
   "source": [
    "Face mesh processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e87b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image filename and drawing specifications\n",
    "file = 'face_image.jpg'\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "# Create a face mesh object\n",
    "with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True,\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5) as face_mesh:\n",
    "\n",
    "    # Read image file with cv2 and process with face_mesh\n",
    "    image = cv2.imread(file)\n",
    "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Define boolean corresponding to whether or not a face was detected in the image\n",
    "face_found = bool(results.multi_face_landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac40fac4",
   "metadata": {},
   "source": [
    "Drawing the mesh tessellation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b6d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if face_found:\n",
    "    # Create a copy of the image\n",
    "    annotated_image = image.copy()\n",
    "    \n",
    "    # Draw landmarks on face\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=results.multi_face_landmarks[0],\n",
    "        connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "            .get_default_face_mesh_tesselation_style())\n",
    "        \n",
    "    # Save image\n",
    "    cv2.imwrite('face_tesselation_only.png', annotated_image)\n",
    "\n",
    "# Open image\n",
    "img = Image.open('face_tesselation_only.png')\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abe6366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f0c292",
   "metadata": {},
   "outputs": [],
   "source": [
    "if face_found:\n",
    "    # Create a copy of the image\n",
    "    annotated_image = image.copy()\n",
    "    \n",
    "    # For each face in the image (only one in this case)\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "        \n",
    "        # Draw the facial contours of the face onto the image\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks,\n",
    "            connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp_drawing_styles\n",
    "                .get_default_face_mesh_contours_style())\n",
    "        \n",
    "        # Draw the iris location boxes of the face onto the image\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks,\n",
    "            connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp_drawing_styles\n",
    "                .get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "\t# Save the image\n",
    "    cv2.imwrite('face_contours_and_irises.png', annotated_image)\n",
    "\n",
    "    # Open image\n",
    "img = Image.open('face_contours_and_irises.png')\n",
    "display(img)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4ec786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3acc977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41398878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38a5fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9d2b282",
   "metadata": {},
   "source": [
    "# Essai 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f043cc5c",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/face-and-hand-landmarks-detection-using-python-mediapipe-opencv/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d896eb3e",
   "metadata": {},
   "source": [
    "Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f91360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8009b939",
   "metadata": {},
   "source": [
    "Initializing Holistic model and Drawing utils for detecting and drawing landmarks on the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f857e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the Holistic Model from Mediapipe and\n",
    "# Initializing the Model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(\n",
    "    static_image_mode=False,         # static images or video stream\n",
    "    model_complexity=1,              # 0, 1, or 2 : it increases landmark accuracy and latency. \n",
    "    smooth_landmarks=True,           # reduce the jitter in the prediction by filtering pose landmarks across different input images\n",
    "    min_detection_confidence=0.5,    # minimum confidence value with which the detection from the person-detection model needs to be considered as successful. Can specify a value in [0.0,1.0]\n",
    "    min_tracking_confidence=0.5)     # minimum confidence value with which the detection from the landmark-tracking model must be considered as successful. Can specify a value in [0.0,1.0]\n",
    "\n",
    "# Initializing the drawng utils for drawing the facial landmarks on image\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdec7f6",
   "metadata": {},
   "source": [
    "STEP-3: Detecting Face and Hand landmarks from the image. Holistic model processes the image and produces landmarks for Face, Left Hand, Right Hand and also detects the Pose of the \n",
    "\n",
    "- Capture the frames continuously from the camera using OpenCV.\n",
    "- Convert the BGR image to an RGB image and make predictions using initialized holistic model.\n",
    "- The predictions made by the holistic model are saved in the results variable from which we can access the landmarks using results.face_landmarks, results.right_hand_landmarks, results.left_hand_landmarks respectively.\n",
    "- Draw the detected landmarks on the image using the draw_landmarks function from drawing utils.\n",
    "- Display the resulting Image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4262096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0) in VideoCapture is used to connect to your computer's default camera\n",
    "capture = cv2.VideoCapture(0)\n",
    " \n",
    "# Initializing current time and previous time for calculating the FPS\n",
    "previousTime = 0\n",
    "currentTime = 0\n",
    " \n",
    "while capture.isOpened():\n",
    "    # capture frame by frame\n",
    "    ret, frame = capture.read()\n",
    " \n",
    "    # resizing the frame for better view\n",
    "    frame = cv2.resize(frame, (800, 600))\n",
    " \n",
    "    # Converting the from BGR to RGB\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    " \n",
    "    # Making predictions using holistic model\n",
    "    # To improve performance, optionally mark the image as not writable to pass by reference.\n",
    "#     image.flags.writable = False\n",
    "    results = holistic_model.process(image)\n",
    "#     image.flags.writable = True\n",
    " \n",
    "    # Converting back the RGB image to BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    " \n",
    "    # Drawing the Facial Landmarks\n",
    "    mp_drawing.draw_landmarks(image,\n",
    "                              results.face_landmarks,\n",
    "                              mp_holistic.FACEMESH_TESSELATION, # mp_holistic.FACE_CONNECTIONS\n",
    "                              mp_drawing.DrawingSpec(color=(255,0,255),\n",
    "                                                     thickness=1,\n",
    "                                                     circle_radius=1),\n",
    "                              mp_drawing.DrawingSpec(color=(0,255,255),\n",
    "                                                     thickness=1,\n",
    "                                                     circle_radius=1)\n",
    "                             )\n",
    " \n",
    "    # Drawing Right hand Land Marks\n",
    "    mp_drawing.draw_landmarks(image,\n",
    "                              results.right_hand_landmarks,\n",
    "                              mp_holistic.HAND_CONNECTIONS\n",
    "                             )\n",
    " \n",
    "    # Drawing Left hand Land Marks\n",
    "    mp_drawing.draw_landmarks(image,\n",
    "                              results.left_hand_landmarks,\n",
    "                              mp_holistic.HAND_CONNECTIONS\n",
    "                             )\n",
    "     \n",
    "    # Calculating the FPS\n",
    "    currentTime = time.time()\n",
    "    fps = 1 / (currentTime-previousTime)\n",
    "    previousTime = currentTime\n",
    "     \n",
    "    # Displaying FPS on the image\n",
    "    cv2.putText(image, str(int(fps))+\" FPS\", (10, 70), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    " \n",
    "    # Display the resulting image\n",
    "    cv2.imshow(\"Facial and Hand Landmarks\", image)\n",
    " \n",
    "    # Enter key 'q' to break the loop\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "# When all the process is done\n",
    "# Release the capture and destroy all windows\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf26427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to access landmarks\n",
    "for landmark in mp_holistic.HandLandmark:\n",
    "    print(landmark, landmark.value)\n",
    " \n",
    "print(mp_holistic.HandLandmark.WRIST.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da341373",
   "metadata": {},
   "source": [
    "# Essai comptage doigts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eea24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import hand_tracking_module as htm\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3,640)\n",
    "cap.set(4,480)\n",
    "\n",
    "\n",
    "path=\"finger\"\n",
    "myList=os.listdir(path)\n",
    "overlayList=[]\n",
    "for impath in [1,2,3,4,5,6]:\n",
    "    image=cv2.imread(f'{path}/{impath}.png')\n",
    "    overlayList.append(image)\n",
    "\n",
    "pTime = 0\n",
    "\n",
    "detector = htm.handDetector(detectionCon=0) #0.75\n",
    "tipIds = [4, 8, 12, 16, 20]\n",
    "tipInfIds = [3, 7, 11, 15, 19]\n",
    "\n",
    "while True:\n",
    "    success,img=cap.read()\n",
    "    img = cv2.flip(img, 1) \n",
    "    img = detector.findHands(img)\n",
    "    lmList = detector.findPosition(img,draw=False)\n",
    "    #print(lmList)\n",
    "    if len(lmList) !=0:\n",
    "        fingers=[]\n",
    "\n",
    "        # Thumb\n",
    "        if lmList[tipIds[0]][1] > lmList[tipIds[0] - 1][1]:\n",
    "            fingers.append(1)\n",
    "        else:\n",
    "            fingers.append(0)\n",
    "\n",
    "        for id in range(1,5):  #y axis\n",
    "            if lmList[tipIds[id]][2] < lmList[tipIds[id]-2][2]:\n",
    "                fingers.append(1)\n",
    "            else:\n",
    "                fingers.append(0)\n",
    "\n",
    "        totalFingers=fingers.count(1)\n",
    "        print(totalFingers)\n",
    "\n",
    "        h,w,c=overlayList[totalFingers].shape\n",
    "        img[0:h,0:w]=overlayList[totalFingers]\n",
    "\n",
    "        cv2.rectangle(img,(20,225),(170,425),(0,255,0),cv2.FILLED)\n",
    "        cv2.putText(img,str(totalFingers),(45,375),cv2.FONT_HERSHEY_PLAIN,10,(255,0,0),25)\n",
    "\n",
    "    cTime=time.time()\n",
    "    fps=1/(cTime-pTime)\n",
    "    pTime=cTime\n",
    "\n",
    "    cv2.putText(img,f'FPS: {int(fps)}',(400,70),cv2.FONT_HERSHEY_PLAIN,3,(255,0,0),3)\n",
    "    \n",
    "    cv2.imshow(\"Image\", img)\n",
    "    key = cv2.waitKey(1) \n",
    "    if key & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "# Destroy all the windows\n",
    "cv2.destroyAllWindows()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d5f1dc",
   "metadata": {},
   "source": [
    "# face swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77526d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import cv2\n",
    "\n",
    "from face_landmarks_extractor import MediapipeFaceLandmarksExtractor, DlibLandmarksExtractor\n",
    "from face_warper import face_warp\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"-src\", \"--src_face\", action=\"store\", type=str, required=True,\n",
    "                    help=\"Source face to swap from\")\n",
    "    ap.add_argument(\"-dest\", \"--dest_face\", action=\"store\", type=str, required=True,\n",
    "                    help=\"Dest face to swap the source face to\")\n",
    "    ap.add_argument(\"-o\", \"--output\", action=\"store\", type=str, default=\"out.jpg\",\n",
    "                    help=\"dir containing background images used for augmentation\")\n",
    "    ap.add_argument(\"-l\", \"--landmarks\", action=\"store\", type=str, default=\"mediapipe\",\n",
    "                    choices=[\"mediapipe\", \"dlib\"],\n",
    "                    help=\"dir containing background images used for augmentation\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    img_src = cv2.cvtColor(cv2.imread(args.src_face), cv2.COLOR_BGR2RGB)\n",
    "    img_dest = cv2.cvtColor(cv2.imread(args.dest_face), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    if args.landmarks == \"dlib\":\n",
    "        landmarks_extractor = DlibLandmarksExtractor(device=\"cuda\")\n",
    "    else:\n",
    "        landmarks_extractor = MediapipeFaceLandmarksExtractor(max_num_faces=1)\n",
    "\n",
    "    landmarks_src = landmarks_extractor(img_src)[\"face_landmarks\"][0].landmarks\n",
    "    landmarks_dest = landmarks_extractor(img_dest)[\"face_landmarks\"][0].landmarks\n",
    "\n",
    "    warp_res = face_warp(\n",
    "        img_src,\n",
    "        img_dest,\n",
    "        landmarks_src,\n",
    "        landmarks_dest,\n",
    "    )\n",
    "\n",
    "    cv2.imwrite(args.output, cv2.cvtColor(warp_res[\"dest_warp\"], cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce27b29",
   "metadata": {},
   "source": [
    "# Essai Luciana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31604605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import acv_functions\n",
    "acv_functions.groucho()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20d8703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a408a164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f007b06a",
   "metadata": {},
   "source": [
    "# Menu souris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc18c8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "#read image\n",
    "img = cv2.imread('data/tulips.jpg')\n",
    "  \n",
    "# show image\n",
    "cv2.imshow('image', img)\n",
    "   \n",
    "#define the events for the mouse_click.\n",
    "def mouse_click(event, x, y, flags, param):\n",
    "      \n",
    "    # to check if left mouse button was clicked\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "          \n",
    "        # font for left click event\n",
    "        font = cv2.FONT_HERSHEY_TRIPLEX\n",
    "        LB = 'Left Button'\n",
    "          \n",
    "        # display that left button \n",
    "        # was clicked.\n",
    "        cv2.putText(img, LB, (x, y), \n",
    "                    font, 1, \n",
    "                    (255, 255, 0), \n",
    "                    2) \n",
    "        cv2.imshow('image', img)\n",
    "          \n",
    "          \n",
    "    # to check if right mouse button was clicked\n",
    "    if event == cv2.EVENT_RBUTTONDOWN:\n",
    "           \n",
    "        # font for right click event\n",
    "        font = cv2.FONT_HERSHEY_SCRIPT_SIMPLEX\n",
    "        RB = 'Right Button'\n",
    "          \n",
    "        # display that right button \n",
    "        # was clicked.\n",
    "        cv2.putText(img, RB, (x, y),\n",
    "                    font, 1, \n",
    "                    (0, 255, 255),\n",
    "                    2)\n",
    "        cv2.imshow('image', img)\n",
    "\n",
    "cv2.setMouseCallback('image', mouse_click)   \n",
    "cv2.waitKey(0)\n",
    "# close all the opened windows.\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92fe419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "227.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
